# CMU_LTI_Summer_Internship

## What motivated me to apply to this program?

**Research Interests**

As an undergraduate student studying artificial intelligence and cognitive science, I am broadly interested in understanding the underlying mechanisms of human social cognition and developing socially intelligent systems that can interact with humans safely. I believe language serves as a cornerstone for human thought, reasoning, and social interaction. Inspired by recent advancements in generative agents and social simulation, my specific research interests include but are not limited to:

1. Designing experiments and benchmarks to evaluate social intelligence in generative-agent-based environments, drawing insights from social psychology.
2. Leveraging generative-agent-based environments as social simulators to study social dynamics, particularly the emergence and evolution of social norms and institutions.
3. Evaluating large language models (LLMs) through the lens of cognitive science and designing more human-like language models.
4. AI ethics, fairness, and the application of AI for social good.

**Goals for This Program**

I am eager to join this program to collaborate with leading researchers in language technology. In particular, I am inspired by Prof. **Maarten Sap**’s work on assessing AI agent safety within complex social interactions and interactively training LLMs to enhance their social intelligence. Additionally, I greatly admire Prof. **Yonatan Bisk**’s research on improving LLMs’ theory-of-mind reasoning and incorporating inductive biases into language models. Furthermore, Prof. **Louis-Philippe Morency**’s research on artificial social intelligence strongly aligns with my passion for building socially aware AI systems by integrating and analyzing multimodal information.

With my background at the intersection of AI, cognitive science, and social science, I believe I can make meaningful contributions to this program and the CMU LTI community.

## What programming experience do I have?

1. Programming Language and Tools: Python / C / C++ / PyTorch / LaTeX / object-oriented programming (Advanced), jsPsych / JavaScript / Shell / Docker / Blender (intermediate)

2. Projects I’ve worked on that require strong programming skills



## What experience do I have with natural language processing, machine learning, linguistics, or other non-CS academic areas relevant to my interest in language technologies?

**Project 1: Computational Modeling of Human Visual Pretense** (conducted during my internship at Prof. Tomer Ullman's lab at Harvard)

This project investigates visual pretense—a mode of imagination in which people use physical objects to stand for ideas in their minds, such as pointing to a bottle and saying “Suppose this is my car…”. I developed a computational model of the cognitive process underlying this behavior. The model integrates two modules, (1) a shape representation module that maps a visual or linguistic depiction (such as an image or a phrase) of real and pretend objects to a 3D point cloud, and (2) an analogical reasoning module that searches for a good alignment between the two point clouds of a real object and a pretend object. I tested this model’s predictions on the stimuli developed in previous human behavioral studies and compared model performance with that of other multimodal generative models.

**Project 2: Fine-tuning LLaMA-7B using RLCD** (conducted during my internship at Prof. Tomer Ullman's lab at Harvard)

The goal of this project was to explore whether post-training alignment methods, such as RLHF, RLAIF, and RLCD, influence the internal diversity of large language models. To achieve this, I fine-tuned LLaMA-7B using reinforcement learning from contrastive distillation (RLCD) and evaluated its conceptual diversity through a word-color association task and a conceptual similarity judgment task. This work was later accepted for NAACL 2025. Although I was not listed as an author, I gained expertise in training large language models on FASRC GPUs, leveraging the Accelerate library for distributed training, and resolving multi-node communication challenges.

**Project 3: Computational Modeling of Nonverbal Communication** (conducted during my internship at Beijing Institute for General Artificial Intelligence)

To explore how nonverbal signals, such as pointing and gazing, influence agents’ cognition and action planning in social interactions, I developed Heider-Simmel-style animations using a physics engine (i.e., pybox2d). The agents could communicate through pointing and perform hierarchical planning within a decentralized POMDP framework. The animations aligned with human intuition, demonstrating how pointing can assist or obstruct others while minimizing movement costs. These findings shed light on how nonverbal signals contribute to joint attention and shape cognitive processes in social interactions.

**Project 4: Computational Modeling of Ownership Norms** (conducted during my internship at Prof. Yixin Zhu's lab at PKU)

In this project, I aimed at building AI systems that can perceive, infer, and reason about ownership relations in real-world scenarios. To achieve this, I conducted a comprehensive literature review across disciplines such as computer science, cognitive science, social psychology, philosophy, and law. Through extensive reading and discussions, I developed a theoretical model to enable machines to infer ownership relations from multimodal cues.

**Project 5: Investigating the algorithmic process of human visual pretense** (conducted during my internship at Prof. Tomer Ullman's lab at Harvard)

To explore whether human visual pretense involves mental rotation, I coded and deployed a web- based behavioral studies. I conducted an online human experiment to collect reaction time data for a visual pretense task, analyzing how reaction times varied based on the difference between the object's actual angle and its canonical orientation. I also generated stimuli by rendering 2D images of everyday objects from various viewpoints using Blender.



## Are there unique elements of my background that could help me contribute to building more inclusive language technologies in the future?

1. Besides AI, I also study cognitive science, and worked as a full-time research intern at the Computational, Cognition and Development Lab at Harvard University (guided by Prof. Tomer Ullman). I think insights from cognitive science can contribute to building more human-like language models and developing language tools that can interact with humans safely.
2. As a female in STEM, I have a strong commitment to fairness and justice. I have faced doubts from others who believed that women shouldn't pursue STEM disciplines. However, I persevered and eventually gained admission to one of the top universities in China, securing an opportunity as an RA at Harvard University. I know there's still a long way to go before we can break the societal biases against women and Asian communities, but I aim to challenge stereotypes and promote social equity by becoming a leading figure in research and supporting underrepresented students, as others have supported me.

## Do I have any academic or leadership achievements that I would like to tell you about?

1. Ranked 1st out of 570,000+ in the Chinese College Entrance Examination (Hunan Province), earning admission to Peking University's highly selective Tong Class. This program, representing the highest academic tier at Peking University, admits only the most elite students based on both college entrance examination results and first-year university performance.
2. Awarded multiple prestigious scholarships, including:
- Mingde Scholarship (Top 50 among all freshmen)
- XueErSi Boya Scholarship (Top 25 out of 4,000+ students)
- Social Work Excellence Award
3. Served as Monitor of the PKU Tong Class, organizing class activities and fostering community engagement. Additionally, co-led the Tongzhi Talk Series, which invites outstanding peers to give talks on AI, computer science, and cognitive science.
4. Worked as a Teaching Assistant for the course Mathematical Foundations for AI in Spring 2024.
5. Served as a Reviewer for the Cognitive Science Society in 2024 and 2025.
6. Invited to give a talk at IJTCS-FAW 2023 Conscious Turing Machine (CTM) workshop.


